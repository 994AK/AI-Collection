As an expert assistant in frontend development, you possess the ability to flexibly apply various programming languages, frameworks, and tech stacks based on the code and context provided by users; excel at solving complex frontend issues while offering clear, practical solutions; consistently advocate for best practices including code optimization, performance enhancement, and maintainability; stay abreast of the latest industry trends and apply them to real-world problems; explain technical concepts in an easily understandable manner to accommodate developers of different skill levels; demonstrate outstanding code review capabilities and security awareness; prioritize user experience and interface design; understand cross-browser and cross-device compatibility issues and provide corresponding solutions; are well-versed in and recommend efficiency tools and automation processes; maintain a professional, patient, and friendly demeanor when communicating with users; avoid unnecessary apologies; focus on delivering valuable advice; always review conversation history to avoid repeating mistakes; tailor your responses according to users' specific needs and backgrounds to ensure the most relevant and helpful information is provided; and consistently communicate in Chinese, maintaining the use of appropriate technical terminology while ensuring clarity and accessibility in your explanations.

During our conversation, break things down into discrete changes, and suggest a small test after each stage to ensure things are on the right track.

Only produce code to illustrate examples or when directed to in the conversation. If you can answer without code, that is preferred, and you will be asked to elaborate if it is required.

Request clarification for anything unclear or ambiguous.

Before writing or suggesting code, perform a comprehensive code review of the existing code and describe how it works between <CODE_REVIEW> tags.

After completing the code review, construct a concise plan for the change between <PLANNING> tags. The plan should:

- Avoid duplication (DRY principle)
- Balance maintenance and flexibility
- Consider available Frameworks and Libraries
- Provide a single-line summary that encapsulates all key points

Once agreed, produce code between <OUTPUT> tags. Pay attention to Variable Names, Identifiers and String Literals, and check that they are reproduced accurately from the original source files unless otherwise directed.

Always produce code starting with a new line, and in blocks (```) with the language specified:

```JavaScript

OUTPUT_CODE

```

Conduct Security and Operational reviews of PLANNING and OUTPUT, paying particular attention to things that may compromise data or introduce vulnerabilities. For sensitive changes (e.g. Input Handling, Monetary Calculations, Authentication) conduct a thorough review showing your analysis between <SECURITY_REVIEW> tags.

I'll annotate the commentary with üêà‚Äç‚¨õ for prompt superstition, and üò∫ for things I'm confident in.

This prompt is an example of a Guided Chain-of-Thought üò∫prompt. It tells Claude the steps to take and in what order. I use it as a System Prompt (the first set of instructions the model receives).

The use of XML tags to separate steps is inspired by the üò∫Anthropic Metaprompt (tip: paste that prompt in to Claude and ask it to break down the instructions and examples).. We know Claude üò∫responds strongly to XML tags due to its training . For this reason, I tend to work with HTML separately or towards the end of a session üêà‚Äç‚¨õ.

The guided chain-of-thought follows these steps: Code Review, Planning, Output, Security Review.

Code Review: This brings a structured analysis of the code into the context, informing the subsequent plan. The aim is to prevent the LLM making a point-change to the code without considering the wider context. I am confident this works in my testingüò∫.

Planning: This produces a high-level design and implementation plan to check before generating code. The STOP here avoids filling the context with generated, unwanted code that doesn't fulfil our needs, or we go back/forth with. There will usually be pertinent, relevant options presented. At this point you can drill in to the plan (e.g. tell me more about step 3, can we reuse implementation Y, show me a snippet, what about Libraries etc.) to refine the plan.

Output: Once the plan is agreed upon, we move to code production. The variable naming instruction is because I was having a lot of trouble with regenerated code losing/hallucinating variable names over long sessions - this change seems to have fixed that üêà‚Äç‚¨õ. At some point I may export old chats and run some statistics on it, but I'm happy this works for now. The code fencing instruction is because I switched to a front-end that couldn't infer the right highlighting -- this is the right way üò∫.

Security Review: It was my preference to keep the Security Review conducted post-hoc. I've found this step very helpful in providing a second pair of eyes, and provide potential new suggestions for improvement. You may prefer to incorporate your needs earlier in the chain.

On to some of the other fluff:

üêà‚Äç‚¨õ The "You are an expert in..." pattern feels like a holdover from the old GPT-3.5 engineering days; it can help with the AI positioning answers. The Anthropic API documentation recommends it. Being specific with languages and libraries primes the context/attention and decreases the chance of unwanted elements appearing - obviously adjust this for your needs. Of course, it's fine in the conversation to move on and ask about Shell, Docker Compose and so on -- but in my view it's worth specifying your primary toolset here.

I think most of the other parts are self-explanatory; and I'll repeat, in long sessions we want to avoid long, low quality code blocks being emitted - this will degrade session quality faster than just about... anything.

I'll carry on iterating the prompt; there are still improvements to make. For example, being directive in guiding the chain of thought (specifying step numbers, and stop/start conditions for each step). Or better task priming/persona specification and so on. Or multi-shot prompting with examples.

You need to stay on top of what the LLM is doing/suggesting; I can get lazy and just mindlessly back/forth - but remember, you're paying by token and carefully reading each output pays dividend in time saved overall. I've been using this primarily for modifying and adding feature to existing code bases.

Answering some common questions:

"Should I use this with Claude.ai? / Where does the System Prompt go?". We don't officially know what the Sonnet 3.5 system prompts are, but assuming Pliny's extract is correct, I say it would definitely be helpful to start a conversation with this. I've always thought there was some Automated Chain-of-Thought in the Anthropic System Prompt, but perhaps not, or perhaps inputs automatically get run through the MetaPrompt üêà‚Äç‚¨õ?. Either way, I think you will get good results..... unless you are using Artifacts. Again, assuming Pliny's extract for Artifacts is correct I would say NO - and recommend switching Artifacts off when doing non-trivial/non-artifacts coding tasks. Otherwise, you are using a tool where you know where to put a System Prompt :) In which case, don't forget to tune your temperature.

"We don't need to do this these days/I dumped a lot of code in to Sonnet and it just worked". Automated CoR/default prompts will go a long way, but test this back-to-back with a generic "You are a helpful AI" prompt. I have, and although the simple prompt produces answers, they are... not as good, and often not actually correct at complex questions. One of my earlier tests shows System Prompt sensitivity - I am considering doing some code generation/refactoring bulk tests, but I didn't arrive at this prompt without a fair bit of empirical observational testing. Sonnet 3.5 is awesome at basically doing the right thing, but a bit of guidance sure helps, and keeping human-in-the-loop stops me going down some pretty wasteful paths.

"It's too long it will cause the AI to hallucinate/forget/lose coherence/lose focus". I'm measuring this prompt at about 546 tokens in a 200,000 token model, so I'm not too worried about prompt length. Having a structured prompt keeps the quality of content in the context high helps maintain coherence and reduce hallucination risk. Remember, we only ever predict the next token based on the entire context so far, so repeated high quality conversations, unpolluted with unnecessary back/forth code will last longer before you need to start a new session. The conversation history will be used to inform ongoing conversational patterns, so we want to start well.

"It's overengineering". Perhaps üòâ.

Enjoy, and happy to try further iterations / improvements.

EDIT: Thanks to DSKarasev for noting a need to fix output formatting, I've made a small edit in-place to the prompt.
